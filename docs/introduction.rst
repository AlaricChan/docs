RackHD Overview
===================
In a Converged Infrastructure Platform (CIP) architecture, RackHD software provides hardware management and orchestration (M&O). It serves as an abstraction layer between other M&O layers and the underlying physical hardware. Developers can use the RackHD API to create a user interface that serves as single point of access for managing hardware services regardless of the specific hardware in place.

Once RackHD is installed on the managed CIP platform, it has the ability to discover the existing hardware resources, catalog each component, and retrieve detailed telemetry information from each resource. The retrieved information can then be used to perform low-level hardware management functions for each resource, such as BIOS configuration, OS installation, and firmware management.

RackHD sits between the other M&O layers and the underlying physical hardware devices. User interfaces at the higher M&O layers can request hardware services from RackHD. RackHD handles the details of connecting to and managing the hardware devices.

Management Paradigm
----------------------------
RackHD uses the concept of a ‘neighborhood’ to represent the compute nodes, storage nodes, network switches, and smart PDUs under management. In most cases, a neighborhood corresponds to one physical rack, although a neighborhood can span several racks. The physical compute, storage, and network devices in a neighborhood are logically represented as ‘elements’. For example, a compute element can represent a CPU, while a storage element might refer to a storage disk.
Elements can be managed as separate entities or they can be combined and managed as one larger entity. For example, multiple storage elements can be combined to create a single storage pool.

Hardware Management
---------------------------

To implement the API requests received from the other M&O layers, RackHD primarily uses out-of-band network management to communicate with hardware devices using platform APIs (IPMI, SNMP, etc.). For functionality that is not available out-of-band, RackHD can communicate with the in-band data network.

Features
------------------------

======================== ============================================================================
Feature                   Description
======================== ============================================================================
Discovery and Cataloging  Discovers the compute, network, and storage resources and catalogs their attributes and capabilities.
Telemetry and Genealogy   Telemetry data includes genealogical details, such as hardware, revisions, serial numbers, and date of manufacture
Device Management         Powers devices on and off. Manages the firmware, power, OS installation, and base configuration of the resources.
Configuration             Configures the hardware per application requirements. This can range from the BIOS configuration on compute devices to the port configurations in a network switch.
Provisioning              Provisions a node to support the intended application workflow, for example lays down ESXi from an image repository.
                          Reprovisions a node to support a different workload, for example changes the ESXi platform to Bare Metal CentOS.
Firmware Management       Manages all infrastructure firmware versioning.
Logging                   Log information can be retrieved for particular elements or collated into a single timeline for multiple elements within the management neighborhood.
Environmental Monitoring  Aggregates environmental data from hardware resources. The data to monitor is configurable and can include power information, component status, fan performance, and other information provided by the resource.
Fault Detection           Monitors compute and storage devices for both hard and soft faults. Performs suitable responses based on pre-defined policies.
Analytics Data            Data generated by environmental and fault monitoring can be provided to analytic tools for analysis, particularly around predictive failure.
======================== ============================================================================




Goals
-----------------------------------------

The primary goals of RackHD are to provide REST APIs and live data feeds to enable automated solutions
for managing hardware resources. The technology and architecture are built to provide a platform
agnostic solution.

The combination of these services is intended to provide a REST API based service to:

* Install, configure, and monitor bare metal hardware, such as compute servers, power distribution
  units (PDUs), direct attached extenders (DAE) for storage, and network switches.
* Provision, erase, and reprovision a compute server's OS.
* Install and upgrade firmware for qualified hardware.
* Monitor and alert bare metal hardware through out-of-band management interfaces.
* Provide RESTful APIs for convenient access to knowledge about both common and vendor-specific hardware.
* Provide pub/sub data feeds for alerts and raw telemetry from hardware.

The RackHD Project
-----------------------------------------

RackHD is an open source project available under the Apache 2.0 license (or
compatible sub-licenses for library dependencies). It is housed at https://github.com/RackHD.
The code for RackHD is a combination of Javascript/Node.js and C. The project is a collection
of libraries and applications intended to be deployed together to provide a solution that can be used
either standalone, or as a technology to be included and embedded in larger applications.

The RackHD documentation is also housed on GitHub
and hosted at http://rackhd.readthedocs.org/en/latest/.

Project History
~~~~~~~~~~~~~~~~~~~~~

The project started with the goal of providing a consistent and clear mechanism to
perform hardware inventory and firmware upgrades to commodity white-box servers.
Existing open source solutions do an admirable job of inventory and bare OS
provisioning, but the ability to upgrade firmware was beyond the technology
stacks currently available (i.e. `xCat`_, `Cobbler`_, `Razor`_ or `Hanlon`_).
The expansion to utilizing an event-based workflow engine in alignment with the
services providing classical PXE booting makes it possible to architect a number
of different deployment configurations as described in :doc:`how_it_works` and
:doc:`packaging_and_deployment`.

The motivation for starting RackHD
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The original problem we were sorting out how to solve was how to handle some of
the complexities of firmware and BIOS updates in a fully automated fashion. The
dirty secret of the computer hardware industry is that many of those tools are
far from automated, requiring tremendous manual effort and often a physical human
presence in the datacenter to accomplish some tasks. We aimed to automate as
absolutely much of that effort as possible.

The first problem was getting to the tools to the machine - where there is
pretty good support. PXE booting was the obvious choice for a platform agnostic
mechanism, as it’s fairly industry standard, and while not without it’s quirks,
it’s reasonably consistent and most platforms conform to the specs that Intel
wrote for the process. Perfect - Cobbler, Razor, etc all exist - Razor (or it’s
clone/rewrite Hanlon) have lovely APIs, and even better - they have a microkernel
for doing what is essentially arbitrary tasks on the remote machine - in the
case of Razor and Hanlon, what those tasks are is baked into the microkernel -
but still a tremendous capability to leverage.

The second problem is what really hit us - When it came to needing needing to do
a process that  involved the steps such as:

 * PXE boot the server
 * interrogate the hardware, see if we’re at the right version of firmware
 * if not, flash the firmware to the version we want
 * reboot (mandated by things like BIOS and BMC flashing)
 * PXE boot again
 * interrogate the hardware
 * make sure we’re at the right version of firmware
 * SCORE!

To achieve this, the existing systems (Cobbler, Razor, etc) needed another
evel of orchestration - resetting what we PXE boot, interacting with data from
he machine in question, and making some logical decisions. This sequence of
needing multiple steps that involved PXE booting is what ultimately led to the
project RackHD.

At the highest level RackHD couples standard open source tools and daemons used
for PXE booting with a declarative, event-based workflow engine. We leveraged
the same concept that Razor and Hanlon used in setting up and booting a
microkernel, and rather than just enabling it to do one-shot activities
(whatever you built into the microkernel), we went ahead and added a simple
agent and some communications mechanisms so that the workflow engine interact
could specify tasks to be accomplished on the target machine. Zero-ing out
disks, interrogating the PCI bus, or resetting the IPMI settings through a
hosts’s internal KCS channel. With this remote agent to workflow integration,
we also optimized the path for interrogating and gathering data - leveraging
existing linux tools and parsing the outputs, sending that information back to
be stored as relatively free-form JSON data structures.

We extended the workflow engine to support polling out of band interfaces to
capture information about the sensors, and whatever generally useful information
we could get from those standard interfaces via IPMI. In RackHD these become
“pollers”, with the intention of periodically capturing telemetry data from
the hardware interfaces.

What RackHD is good at
^^^^^^^^^^^^^^^^^^^^^^^^

The obvious mechanism of provisioning an OS is one of the more straightforward
workflows you can image, and fundamentally RackHD is really focused on being
the lowest level of automation supporting interrogating hardware, setting a
“personality” onto it (in the form of an OS), providing consistent REST based
API’s for controlling that hardware - agnostic of hardware vendor, and for
using the pollers to capture telemetry - create “live data feeds” that can be
provided via a pub/sub style interface.

As we went through use cases and expanded features, we made the capability for
the workflow engine to react to what it discovered - what we call “SKU” support,
dynamic rendering of templates for OS installs, and passing of variables and
data from the APIs that invoke workflow through to the configuration files that
drive OS installs - like a kickstart or debseed file.

While we have a number of workflows in our code repository as examples of how
you can do a variety of actions, the real power of the system is in being able
to create your own workflows - and submit those through the REST API. So you
can define arbitrary workflows for your needs, specific to your hardware if
needed, to accomplish your automation goals.


Where we stopped/What RackHD doesn’t do
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We intentionally stopped at two conceptual boundaries - the first, we didn’t
attempt to replicate all the work and effort that’s gone into software
configuration management systems. Ansible, Puppet, Chef, and so forth have a
long history of dealing with a lot of that complexity, and doing it pretty well.
We made sure our workflow system could integrate seamlessly with those kinds of
systems - making a call to register a machine with a Puppet or Chef service, or
in the case of ansible, some example hooks for how to invoke a playbook or
arbitrary script on the remote machine.

The second - we intentionally made RackHD a comparatively passive system. You
can embed a lot of logic in a workflow, but we stopped short of building in more
complex logic that amounting to functions more commonly done as scheduling -
choosing which machines to install with what OS, etc. We expect that someone,
or some thing, will be making those relevant choices - a layer above hardware
management and orchestration that we saw as “infrastructure orchestration and
management”. We documented and exposed all of the events around the workflow
engine to be utilized, extended, and even incorporated by an infrastructure
management system - but we didn’t take RacKHD directly into that layer.

Project Comparison
~~~~~~~~~~~~~~~~~~~~~
Comparison to other open source technologies:

**Cobbler comparison**

* Grand-daddy of open source tools to enable PXE imaging
* Original workhorse of datacenter PXE automation
* XML-RPC interface for automation, no REST interface
* No dynamic events or control for TFTP, DHCP
* Extensive manual and OS level configuration needed to utilize
* One-shot operations - not structured to change personalities (OS installed) on
  a target machine, or multiple reboots to support some firmware update needs
* No workflow engine or concept of orchestration with multiple reboots

**Razor/Hanlon comparison**

* HTTP wrapper around stock open source tools to enable PXE booting (DHCP,
  TFTP, HTTP)
* Extended beyond Cobbler concepts to include microkernel to interrogate remote
  host and enable some pre-OS-install interactions or diagnostics
* No dynamic events or control for TFTP, DHCP
* Catalog and policy are roughly equivalent to RackHD default/discovery workflow
  and SKU mechanism, but oriented on single OS deployment for a piece or type
  of hardware
* Focused on hardware inventory to choose and enable OS installation
* No workflow engine or concept of orchestration with multiple reboots
* Tightly bound to and maintained by Puppet
* Forked variant `Hanlon`_ used for Chef Metal driver

**xCat comparison**

* HPC Cluster Centric tool focused on IBM supported hardware
* Firmware update features restricted to IBM/Lenovo proprietary hardware where
  firmware was made to "one-shot-update", not explicitly requiring a reboot
* Has no concept of workflow or sequencing
* Has no obvious mechanism for failure recovery
* Competing with Puppet/Chef/Ansible/cfEngine to own config management story
* Extensibility model tied exclusively to Perl code
* REST API is extremely light with focus on CLI management
* Built as a master controller of infrastructure vs an element in the process

.. _Cobbler: http://cobbler.github.io
.. _Razor: https://github.com/puppetlabs/razor-server
.. _Hanlon: https://github.com/csc/Hanlon
.. _xCat: http://xcat.org
